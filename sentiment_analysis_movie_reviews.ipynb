{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMB5CZgZm4l4LZluOKEcYvI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoFzcpNHNl6g"
      },
      "outputs": [],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import contractions\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "1vvNMxo4N4IV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "MP_e99akN77L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the training data\n",
        "#Download the file from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "#Upload the file to your environment\n",
        "file_path = './IMDB_Dataset.csv' #Change the path according to your environment\n",
        "imdb_data = pd.read_csv(file_path)\n",
        "print(imdb_data.shape)\n",
        "print(imdb_data.head(25))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N0vRSJInN-vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and loading stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "U4o42e4pORis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the sentiments \"positive\" and \"negative\" as numeric values\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "# Ensure data types are correct\n",
        "imdb_data['sentiment'] = imdb_data['sentiment'].astype(int)"
      ],
      "metadata": {
        "id": "H2L75bv_OUzx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to clean the training data by removing unnecessary things\n",
        "def clean_text(text):\n",
        "    text = re.sub('<br />', ' ', text)  # HTML tag removal\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # removing all numeric values.\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = contractions.fix(text)  #Contractions like don't are expanded to do not\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words]) #Stopword removal\n",
        "    return text"
      ],
      "metadata": {
        "id": "4im5NlKWOYPg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to the reviews\n",
        "imdb_data['review'] = imdb_data['review'].apply(clean_text)\n",
        "\n",
        "# check the result\n",
        "imdb_data.head(5)"
      ],
      "metadata": {
        "id": "coZPhH95OaGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data into 85% training and 15% testing data\n",
        "X = imdb_data['review']\n",
        "y = imdb_data['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "print(f\"Training set: {len(X_train)} reviews and {len(y_train)} sentiments\")\n",
        "print(f\"Testing set: {len(X_test)} reviews and {len(y_test)} sentiments\")"
      ],
      "metadata": {
        "id": "Ey5m3-MTOfTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=30000)  # Use the first 30,000 words\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "maxlen = 150  # The maximum review lenght is set to 150 for easier processing\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "print(f\"Padded training data shape: {X_train_pad.shape}\")\n",
        "print(f\"Padded testing data shape: {X_test_pad.shape}\")"
      ],
      "metadata": {
        "id": "jul0UbTdO2q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the embedding dimensions, max vocab size, and input length\n",
        "embedding_dim = 200\n",
        "max_vocab_size = 30000\n",
        "maxlen = 150\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(0.01))),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=1)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot training and validation accuracy/loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Model Accuracy')\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Model Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test_pad)\n",
        "predictions = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# Display a few predictions alongside true labels\n",
        "print(\"Sample Predictions:\")\n",
        "for i in range(10):\n",
        "    print(f\"Review: {X_test.iloc[i][:100]}...\")\n",
        "    print(f\"True Sentiment: {y_test.iloc[i]}, Predicted Sentiment: {predictions[i]}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "BNgdDlX7O5jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess handwritten reviews\n",
        "def preprocess_review(review, tokenizer, maxlen):\n",
        "\n",
        "    # Clean the review\n",
        "    review = re.sub('<br />', ' ', review)  # Remove HTML tags\n",
        "    review = re.sub(r'[^a-zA-Z\\s]', '', review)  # Keep only letters\n",
        "    review = review.lower()  # Convert to lowercase\n",
        "    review = contractions.fix(review)  # Expand contractions\n",
        "    review = ' '.join([word for word in review.split() if word not in stop_words])  # Remove stopwords\n",
        "\n",
        "    # Tokenize and pad the review\n",
        "    review_seq = tokenizer.texts_to_sequences([review])\n",
        "    review_pad = pad_sequences(review_seq, maxlen=maxlen)\n",
        "    return review_pad\n",
        "\n",
        "# Function to predict sentiment for a list of reviews\n",
        "def predict_reviews(reviews, model, tokenizer, maxlen):\n",
        "    results = []\n",
        "    for review in reviews:\n",
        "        # Preprocess and predict\n",
        "        processed_review = preprocess_review(review, tokenizer, maxlen)\n",
        "        prediction = model.predict(processed_review, verbose=0)\n",
        "        sentiment = \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
        "        results.append((review, sentiment))\n",
        "    return results\n",
        "\n",
        "# Example list of handwritten reviews\n",
        "handwritten_reviews = [\n",
        "    \"I absolutely loved this movie! The acting was great and the story was so touching.\",\n",
        "    \"This film was a complete waste of time. The plot was terrible, and the acting was even worse.\",\n",
        "    \"It had some good moments, but overall, it was just okay.\",\n",
        "    \"One of the best movies I've seen in a long time. Highly recommended!\",\n",
        "    \"I couldn't finish watching it. It was too boring.\",\n",
        "    \"I absolutely hated this movie! The acting was bad and the story was so boring.\",\n",
        "    \"The movie had some interesting twists, but the pacing was too slow for my liking.\",\n",
        "    \"A masterpiece! The cinematography was stunning, and the storyline was gripping.\",\n",
        "    \"It was alright, but I expected more. It didn't live up to the hype.\",\n",
        "    \"An emotional rollercoaster. I was on the edge of my seat the entire time.\",\n",
        "    \"The special effects were fantastic, but the story didn't resonate with me.\",\n",
        "    \"I found the plot to be too predictable and lacking any real excitement.\",\n",
        "    \"An entertaining film, but not something I'd watch again.\",\n",
        "    \"I loved the soundtrack! It really added to the atmosphere of the movie.\",\n",
        "    \"A solid movie, but it didn't stand out compared to others in the genre.\",\n",
        "    \"Too much CGI for my taste. It felt like the plot was secondary to the effects.\",\n",
        "    \"The ending was so satisfying! Everything came together beautifully.\",\n",
        "    \"It had its moments, but overall, it was too cheesy for me.\",\n",
        "    \"A visually stunning film with a powerful performance from the lead actor.\",\n",
        "    \"The pacing was uneven, but I enjoyed the overall concept of the film.\",\n",
        "    \"One of those films that sticks with you long after it's over. Truly impactful.\",\n",
        "    \"I wasn't a fan of the humor. It felt forced and out of place in the storyline.\",\n",
        "    \"The direction was brilliant, and the performances were top-notch.\",\n",
        "    \"It was a fun, lighthearted movie, perfect for a family night.\",\n",
        "    \"The plot was weak, but the characters were strong and well-developed.\",\n",
        "    \"I didn't understand the hype. It didn't live up to my expectations.\"\n",
        "]\n",
        "\n",
        "# Test the model with handwritten reviews\n",
        "handwritten_results = predict_reviews(handwritten_reviews, model, tokenizer, maxlen)\n",
        "\n",
        "# Display the results\n",
        "print(\"Predictions for Handwritten Reviews:\")\n",
        "for i, (review, sentiment) in enumerate(handwritten_results):\n",
        "    print(f\"Review {i + 1}: {review[:100]}...\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "JWxIjylMSGjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
